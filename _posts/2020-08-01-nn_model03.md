---
layout: post
title: Python神经网络模型（三），Auto-Encoders
categories: [Python, NNs]
mathjax: true
---

自编码器  
观测样本 $x\to$ 隐含变量 $z\to$ 生成样本 $\hat x$

<!-- more -->
## VAE(Variational Auto-Encoder)
1. $q(z \vert x)$变分函数代替后验概率$p(z \vert x)$，$q(z \vert x)$越接近$p(z \vert x)$，则$KL(q(z \vert x) \Vert p(z \vert x))$越小。  
由贝叶斯展开$KL(q(z \vert x) \Vert p(z \vert x))=\log{p(x)} - \int{q(z \vert x) \log{p(x \vert z)} dz} + KL(q(z \vert x) \Vert p(z))$

2. $-\int{q(z \vert x) \log{p(x \vert z)} dz}=-E_{q(z \vert x)}\log p(x \vert z)=H(x,\hat x)$且$\log{p(x)}$确定，  
所以损失函数设计为$loss=H(x,\hat x)+KL(q(z \vert x) \Vert p(z)))$

3. 假设x每一维特征都符合正态分布，计算x的均值$\mu$和方差$\sigma$，并进行随机采样，此过程不可导。  
所以使用重参数方法，从N(0,1)上随机采样$\epsilon$，计算$z=\mu+\sigma\cdot\epsilon$，采样结果$z\sim(\mu,\sigma)$可导。

4. 因为$q(z \vert x) \sim N(\mu,\sigma)$，$p(z) \sim N(0,1)$，所以$KL(q(z \vert x) \Vert p(z)))=KL(N(\mu,\sigma) \Vert N(0,1))$  
由$KL(p1 \Vert p2)=-\frac{1}{2}\sum\limits_{i=1}^n[\log \frac{\sigma1^2}{\sigma2^2}-\frac{\sigma1^2}{\sigma2^2}-\frac{(\mu1-\mu2)^2}{\sigma2^2}+1]$  
得$KL(q(z \vert x) \Vert p(z)))=-\frac{1}{2}\sum\limits_{i=1}^n(\log\sigma^2 - \sigma^2 - \mu^2 + 1)$
   
## β-VAE(Disentangled VAE)
1. z的各维特征解耦

2. $loss = H(x,\hat x) + \beta KL(q(z \vert x) \Vert p(z))),\beta>1$

3. 用信息瓶颈来解释，将$\beta$值设为一个较大值，减少$q(z \vert x)$和$p(z)$的互信息，
剔除了$q(z \vert x)$中对于重建无用的或者说是弱相关的信息，达到各维特征解耦的目的
   
## β-TCVAE(Total Correlation Variational Auto-Encoder)
1. 效果优于β-VAE

2. 拆解$KL(q(z \vert x) \Vert p(z)))$  
$$\begin{align}KL(q(z \vert x) \Vert p(z))) &= KL(q(z,x)\Vert q(z)p(x)),(1)\\\
&+KL(q(z)\Vert\prod_j q(z_j)),(2)\\\
&+\sum_j KL(q(z_j)\Vert p(z_j)),(3)\end{align}$$

3. 第一项是索引编码互信息(index-code MI)，与数据变量和潜变量之间的互信息相关  
第二项是全相关(total correlation)，潜变量空间中变量之间的相互依赖程度  
第三项维度KL散度(dimension-wise KL)，每个潜变量的维度与先验潜变量的维度的偏离

4. 损失函数设计为$loss = H(x,\hat x)+\alpha\(1)+\beta(2)+\gamma(3)$
将$\beta$值设为一个较大值，$(2)$会向0收缩，达到各维特征解耦的目的