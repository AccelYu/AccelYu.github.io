---
layout: post
title: Python深度学习（零），基础
categories: [Python]
mathjax: true
---

机器学习分为传统机器学习和深度学习

<!-- more -->
## 数学概念
### 高等数学
梯度下降法：将损失降到可接受的极小值（局部最优解），所以低维不适用，但高维情况下无限逼近最小值（全局最优解）。  
损失函数为loss，输入为x，输出为y，假设$loss = wx + b - y$，w、b就是要求的神经网络。  
loss与w、b相关，通过截断正态分布取w、b的初值，想要让loss降到最小，就要逼近极小值点。  
梯度是对每个变量的偏微分组成的向量，$\triangle w = - \lambda \frac{\partial}{\partial w}$，$\lambda$为学习率，注意学习率不能太大。  
$w_2 = w_1 + \triangle w$，将$w_2$不停带入进行迭代计算。b同理。

### 线性代数
1. 雅可比矩阵：用来判断高维函数的单调性

2. 特征矩阵：用来判断高维函数的凹凸性

3. 矩阵特征值：几何意义为特征向量只发生伸缩变换

4. 相似矩阵：几何意义为同一个线性变换在不同基（坐标系）下的不同描述

5. 正交矩阵：几何意义为矩阵内代表的向量之间两两互相垂直

6. 左乘和右乘：几何意义为左乘是以世界为坐标系，变换自身位置；右乘是以自身为坐标系，变换世界位置

7. 最小二乘法：推导公式为$w = ({x^T}x)^{-1}{x^T}y$。原理是x输入经w变换得到y。  
简单来说$xw = y \Rightarrow w = x^{-1}y$，但是$x$不一定是个方阵，所以不一定可逆。  
于是将$x^{-1}$转化为$({x^T}x)^{-1}{x^T}$。  
以前多用于传统机器学习，现被淘汰因为深度学习数据量大，求矩阵的逆运算量较大。

8. 奇异值分解（SVD）：推导公式为$A = U \Sigma V^T$。
左奇异矩阵可以用于行数的压缩，推导公式为$X_{d \times n}^\prime = U_{d \times m}^TX_{m \times n}$，将行数从m减到d；
右奇异矩阵可以用于列数的压缩，推导公式为$X_{m \times d}^\prime = X_{m \times n}V_{n \times d}$，将列数从n减到d。
同时使用时取前k个最大的奇异值可以用于压缩图片或者去噪；使用其一可用于PCA降维。

### 概率论
1. 独立性：A，B为随机事件，若同时发生的概率等于各自发生的概率的乘积，则A，B相互独立。两两独立不代表相互独立（波罗梅奥环）。

2. 贝叶斯：推导公式为$P(A|B) = P(A) \frac{P(B|A)}{P(B)}$，P(A)为先验概率，P(A|B)为后验概率，P(B|A)为似然度，P(B)为全概率。
早期使用朴素贝叶斯进行垃圾邮件分类，但效果并不好，因为关键字之间不是相互独立的。

3. 离散分布：0-1分布、二项分布$P(X=k) = C^k_np^k(1-p)^{n-k}$、泊松分布$P(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}$

4. 连续分布：
   1. 均匀分布
   2. 正态分布$f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{- \frac{(x - \mu)^2}{2 \sigma^2}}$、
   3. 标准正态分布$f(x) = \frac{1}{\sqrt{2\pi}}e^{- \frac{x^2}{2}}$，标准差$\sigma=1$，均值$\mu=0$  
其中标准差：$\sigma=\frac{x_i- \overline x}{S}$，$S=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_i-\overline x)^2}$

5. 极大似然估计：将似然函数取得最大值时的概率当作所求概率

6. 最大后验概率估计：极大似然估计结合先验概率分布取得最大值时的概率当作所求概率

### 信息论
1. 信息熵：$H(X) = -\sum\limits_{i=1}^n p(x_i) \log p(x_i)$，描述信源的不确定度。

2. 相对熵：$D_{KL}(P \Vert Q) = \sum\limits_{i=1}^n p(x_i) \log \frac{p(x_i)}{q(x_i)}$，p用来表示样本的真实分布，q用来表示模型所预测的分布。
相对熵用来衡量预测分布与真实分布的差异，在深度学习中用于生成模型（VAE）。

3. 交叉熵：$H(p,q) = -\sum\limits_{i=1}^n p(x_i) \log q(x_i)$。训练数据的分布(信息熵)是给定的，而相对熵=交叉熵-信息熵，所以深度学习中用交叉熵计算损失。

## 传统机器学习
### 监督学习
1. KNN：一个样本在特征空间中的K个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别

### 无监督学习
1. K-means：选择k个样本作为初始质心，形成k个簇，然后重新计算每个簇的质心。迭代直至质心位置趋于收敛

## 深度学习
1. 基于卷积运算的神经网络系统，即卷积神经网络(CNN)。

2. 基于多层神经元的自编码神经网络，包括自编码(Auto encoder)以及近年来受到广泛关注的稀疏编码两类(Sparse Coding)。

3. 以多层自编码神经网络的方式进行预训练，进而结合鉴别信息进一步优化神经网络权值的深度置信网络(DBN)。
