---
layout: post
title: Python神经网络模型（四），GANs
categories: [Python, NNs]
mathjax: true
---

对抗生成网络

<!-- more -->
## GAN(Generative Adversarial Networks)
### 判别器  
1. 目标$maxV(G,D)=E_{x\sim p_r(x)}[log(D(x))] + E_{z\sim p_z(z)}[log(1-D(G(z)))]$

2. $D(x)$越大越好  
反映在loss中，真样本经d判别，结果与1越接近，d就越优秀，损失越小；

3. $1-D(G(z))$越大越好，则$D(G(z))$越小越好  
反映在loss中，假样本经d判别，判断结果与0越接近，d就越优秀，损失越小

### 生成器  
1. 目标$minV(G,D)=E_{x\sim p_g(x)}[log(1-D(x))]$

3. $1-D(x)$越小越好，$D(x)$越大越好  
反映在loss中，假样本经d判别，判断结果与1越接近，g就越优秀，损失越小

### 纳什均衡
1. D：固定$G(x)$  
$V(G,D)=\int p_r(x)\log(D(x)))dx + \int p_g(x)\log(1-D(x)))dx$  
令$p_r(x)=A,p_g(x)=B$  
则$V^\prime=\frac{1}{ln10}\frac{A-(A+B)x}{x(1-x)}$  
令$V^\prime=0$，得极值点（最优判别器）$D^*=\frac{A}{A+B}=\frac{p_r(x)}{p_r(x)+p_g(x)}$

2. G：固定$D(x)$为$D^*$，使用JS散度来衡量两分布的距离  
$$\begin{align}D_{JS}(p_r \Vert p_g) 
&=\frac{1}{2}D_{KL}(p_r \Vert \frac{p_r+p_g}{2})+\frac{1}{2}D_KL(p_g \Vert \frac{p_r+p_g}{2})\\\
&=\frac{1}{2}(log2+\int p_r(x)\log \frac{p_r(x)}{p_r(x)+p_g(x)}dx)\\\
&+\frac{1}{2}(log2+\int p_g(x)\log \frac{p_g(x)}{p_r(x)+p_g(x)}dx)\\\
&=\frac{1}{2}(log4+V(G,D^*))\end{align}$$

### JS散度
1. 优点：解决了顺序不同训练结果不一样的问题（KL散度不对称）

2. 缺点：当生成的初始分布与真实分布无重叠部分，loss保持不变，导致梯度无法更新

### 模式崩溃
判别器只能判断是否生成了正确数据，而对遗失了什么数据不得而知，最终模型会拟合到单一情形中

### 缺陷推导
生成器目标改进$E_{x\sim p_g(x)}[log(1-D(x))] \to E_{x\sim p_g(x)}[-logD(x)],(1)$
在最优判别器$D^*$下$E_{x\sim p_r(x)}[log(D^*(x))] + E_{z\sim p_z(z)}[log(1-D^*(G(z)))]=D_{JS}(p_r \Vert p_g)+2log2$
把KL散度（先g后r）变换成含D^*的形式：  
$$\begin{align}D_{KL}(p_g \Vert p_r) 
&=E_{x\sim p_g(x)}[log\frac{p_g(x)}{p_r(x)}]\\\
&=E_{x\sim p_g(x)}[log\frac{\frac{p_g(x)}{p_r(x)+p_g(x)}}{\frac{p_r(x)}{p_r(x)+p_g(x)}}]\\\
&=E_{x\sim p_g(x)}[log\frac{1-D^*(x)}{D^*(x)}]\\\
&=E_{x\sim p_g(x)}[log1-D^*(x)] - E_{x\sim p_g(x)}[logD^*(x)]\end{align}$$

## WGAN(Wasserstein GAN)
权重剪裁，限制了网络的表达能力

## WGAN-GP(WGAN Gradient Penalty)
