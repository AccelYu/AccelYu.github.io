---
layout: post
title: Python神经网络模型（四），GANs
categories: [Python, NNs]
mathjax: true
---

对抗生成网络

<!-- more -->
## GAN(Generative Adversarial Networks)
### 判别器  
1. 目标$maxV(D)=E_{x\sim p_r(x)}[log(D(x))] + E_{z\sim p_z(z)}[log(1-D(G(z)))]$

2. $D(x)$越大越好，反映在loss中，真样本经d判别，结果与1越接近，d就越优秀，损失越小；

3. $1-D(G(z))$越大越好，则$D(G(z)$越小越好，反映在loss中，假样本经d判别，判断结果与0越接近，d就越优秀，损失越小

### 生成器  
1. 目标minV(G)=E_{x\sim p_g(x)}[log(1-D(x))]$

2. $1-D(x)$越小越好，$D(x)$越大越好，反映在loss中，假样本经d判别，判断结果与1越接近，g就越优秀，损失越小

### 纳什均衡
1. D：$V(D)=\int p_r(x)\log(D(x)))dx + \int p_g(x)\log(1-D(x)))dx$  
固定住$D(x)$，令$p_r(x)=A,p_g(x)=B$  
则$V^\prime=\frac{1}{ln10}frac{A-(A+B)x}{x(1-x)}$  
令$V^\prime=0$，得极值点$D^*=\frac{A}{A+B}=\frac{p_r(x)}{p_r(x)+p_g(x)}$

2. $$\begin{align}D_{JS}(p_r \Vert p_g) 
&=\frac{1}{2}D_KL(p_r \Vert \frac{p_r+p_g}{2})+\frac{1}{2}D_KL(p_g \Vert \frac{p_r+p_g}{2})\\\
&=\frac{1}{2}(log2+\int p_r(x)\log \frac{p_r(x)}{p_r(x)+p_g(x)}dx)+\frac{1}{2}(log2+\int p_g(x)\log frac{p_g(x)}{p_r(x)+p_g(x)}dx)\\\
&=\frac{1}{2}(log4+L(G,D^*)\end{align}$$


### JS散度
1. 优点：解决了KL散度不对称的问题

2. 缺点：当

## WGAN(Wasserstein GAN)


## WGAN-GP(WGAN Gradient Penalty)
