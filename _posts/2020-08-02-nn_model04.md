---
layout: post
title: Python神经网络模型（四），GANs
categories: [Python, NNs]
mathjax: true
---

对抗生成网络

<!-- more -->
## GAN(Generative Adversarial Networks)
### 判别器  
1. 目标$maxV(G,D)=E_{x\sim p_r(x)}[log(D(x))] + E_{z\sim p_z(z)}[log(1-D(G(z)))]$

2. $D(x)$越大越好  
反映在loss中，真样本经d判别，结果与1越接近，d就越优秀，损失越小；

3. $1-D(G(z))$越大越好，则$D(G(z))$越小越好  
反映在loss中，假样本经d判别，判断结果与0越接近，d就越优秀，损失越小

### 生成器  
1. 目标$minV(G,D)=E_{x\sim p_g(x)}[log(1-D(x))]$

3. $1-D(x)$越小越好，$D(x)$越大越好  
反映在loss中，假样本经d判别，判断结果与1越接近，g就越优秀，损失越小

### 纳什均衡
1. D：固定$G(x)$  
$V(G,D)=\int p_r(x)\log(D(x)))dx + \int p_g(x)\log(1-D(x)))dx$  
令$p_r(x)=A,p_g(x)=B$  
则$V^\prime=\frac{1}{ln10}\frac{A-(A+B)x}{x(1-x)}$  
令$V^\prime=0$，得极值点（最优判别器）$D^\ast=\frac{A}{A+B}=\frac{p_r(x)}{p_r(x)+p_g(x)}$

2. G：固定$D(x)$为$D^\ast$，使用JS散度来衡量两分布的距离  
$$\begin{align}D_{JS}(p_r \Vert p_g) 
&=\frac{1}{2}D_{KL}(p_r \Vert \frac{p_r+p_g}{2})+\frac{1}{2}D_KL(p_g \Vert \frac{p_r+p_g}{2})\\\
&=\frac{1}{2}(log2+\int p_r(x)\log \frac{p_r(x)}{p_r(x)+p_g(x)}dx)\\\
&+\frac{1}{2}(log2+\int p_g(x)\log \frac{p_g(x)}{p_r(x)+p_g(x)}dx)\\\
&=\frac{1}{2}(log4+V(G,D^\ast))\end{align}$$

### 缺陷推导
1. 生成器目标改进  
$E_{x\sim p_g(x)}[log(1-D(x))] \to E_{x\sim p_g(x)}[-logD(x)],(1)$  

2. 在最优判别器$D^\ast$下  
$E_{x\sim p_r(x)}[log(D^\ast (x))] + E_{z\sim p_z(z)}[log(1-D^\ast (G(z)))]=2D_{JS}(p_r \Vert p_g)+2log2,(2)$  

3. 把KL散度（先g后r）变换成含$ D^\ast$的形式  
$$\begin{align}D_{KL}(p_g \Vert p_r)
&=E_{x\sim p_g(x)}[log\frac{p_g(x)}{p_r(x)}]\\\
&=E_{x\sim p_g(x)}[log\frac{\frac{p_g(x)}{p_r(x)+p_g(x)}}{\frac{p_r(x)}{p_r(x)+p_g(x)}}]\\\
&=E_{x\sim p_g(x)}[log\frac{1-D^\ast(x)}{D^\ast(x)}]\\\
&=E_{x\sim p_g(x)}log[1-D^\ast(x)] - E_{x\sim p_g(x)}logD^\ast(x),(3)\end{align}$$

4. 由(1)(2)(3)得  
$E_{x\sim p_g(x)}[-logD(x)]=D_{KL}(p_g \Vert p_r)+E_{x\sim p_r(x)}[log(D^\ast (x))]-2D_{JS}(p_r \Vert p_g)-2log2$  
因为$E_{x\sim p_r(x)}[log(D^\ast (x))]-2log2$不依赖于生成器G，应略去  
所以$E_{x\sim p_g(x)}[-logD(x)]=D_{KL}(p_g \Vert p_r)-2D_{JS}(p_r \Vert p_g)$

5. 总结
   1. 第一项最小化生成分布与真实分布的KL散度，而KL散度不对称，对多样性惩罚小，对准确性惩罚大，导致模式崩溃
   生成器倾向于生成准确但是单一的样本
   
   2. 第二项却又要最大化两者的JS散度，非常荒谬，在数值上则会导致梯度不稳定；  
   甚至生成分布与真实分布无重叠部分，loss保持不变，导致梯度消失

## WGAN(Wasserstein GAN)
1. 目标$L(D)=-E_{x\sim p_r}[(D(x)] + E_{\tilde{x}\sim p_g}[D(\tilde{x})]$  
只要满足Lipschitz连续，$\vert f(x)-f(\tilde{x})\vert \leq K(x-\tilde{x})$，$L(D)$就能拟合拟合Wasserstein距离

2. 改动点：
   1. 判别器最后一层去掉sigmoid（因为拟合Wasserstein距离，属于回归任务）
   
   2. 生成器和判别器的loss不取log（根据Lipschitz连续的定义）
   
   3. 每次更新判别器的参数后把它们的绝对值截断到不超过常数c（根据Lipschitz连续的定义）
   
   4. 不使用基于动量的优化算法（包括momentum和Adam），推荐RMSProp

3. 缺点：权重剪裁  
   1. 极大地限制了网络的表达能力
   
   2. 参数容易集中在-c和c两点周围
   
   3. 容易造成梯度弥散或梯度爆炸

## WGAN-GP(WGAN Gradient Penalty)
1. 优化方式：
   1. 使用梯度惩罚而不是权重剪裁，对每个样本独立地施加梯度惩罚
   
   2. 不使用BN，因为它会引入同个batch中不同样本的相互依赖关系

2. 目标$L(D)=-E_{x\sim p_r}[(D(x)] + E_{\tilde{x}\sim p_g}[D(\tilde{x})]
+\lambda E_{\hat{x}\sim p_\hat{x}}[(\Vert\nabla_\hat{x} D(\hat{x})\Vert_2-1)^2]$
   1. 其中$\lambda$为超参数，$\hat{x}=tx+(1-t)\tilde{x}$为真假样本线性插值结果
   
   2. $\Vert\nabla_\hat{x} D(\hat{x})\Vert_2$越接近1越好