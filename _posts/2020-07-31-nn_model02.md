---
layout: post
title: Python神经网络模型（二），RNN
categories: [Python, NNs]
mathjax: true
---

循环神经网络  
RNN每一个时间步上权值共享，大大减少了参数量

<!-- more -->
## LSTM(Long Short-Term Memory)
1. 遗忘门  
$f_t = \sigma (W_f[h_{t-1},x_t] + b_f)$，遗忘的逻辑（门操作）
   
2. 输入门  
$i_t = \sigma (W_i[h_{t-1},x_t] + b_i)$，更新的逻辑（门操作）  
$\widetilde{C_t} = tanh (W_C[h_{t-1},x_t] + b_C)$，候选内容
   
3. 输出门  
$o_t = \sigma (W_o[h_{t-1},x_t] + b_o)$，输出的逻辑（门操作）  
$C_t = f_t \cdot C_{t-1} + i_t \cdot \widetilde{C_t}$，遗忘部分之前内容，更新部分候选内容  
$h_t = o_t \cdot tanh(C_t)$，最终输出

4. 总结  
$C_t$始终贯穿整个网络，远处信息与近处信息相当于同权相加，所以远处信息不会被稀释。
而且梯度相当于是累加的，缓解了梯度弥散的问题。

## GRU(Gated Recurrent Unit)
1. 重置门  
$r_t = \sigma (W_r[h_{t-1},x_t] + b_r)$，重置的逻辑（门操作）

2. 更新门  
$z_t = \sigma (W_z[h_{t-1},x_t] + b_z)$，更新的逻辑（门操作）  
$\widetilde{h_t} = tanh (W[r_t \cdot h_{t-1},x_t])$，重置之前内容，再加上当前内容，得出候选内容  
$h_t = (1-z_t) \cdot h_{t-1} + z_t \cdot \widetilde{h_t}$，遗忘部分之前内容，更新部分候选内容，最终输出

3. 总结  
$h_t$始终贯穿整个网络，相当于LSTM中的$C_t$